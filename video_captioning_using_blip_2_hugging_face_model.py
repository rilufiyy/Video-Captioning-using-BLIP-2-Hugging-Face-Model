# -*- coding: utf-8 -*-
"""Video Captioning Using BLIP-2 Hugging Face Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12z5lVFn0fUsKhvmz4daE7Oa9wnnl6FR4

# Instalation & Load Libraries
"""

import subprocess
import sys

packages = [
    "decord",
    "transformers>=4.50.0",
    "accelerate",
    "torch",
    "pillow",
    "opencv-python-headless",
    "moviepy",
    "sentencepiece",
    "protobuf"
]

for pkg in packages:
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", pkg])
    except:
        pass

print(" All packages installed!\n")

import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import warnings
from transformers import AutoProcessor, AutoModelForCausalLM, Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image
import decord
from decord import VideoReader, cpu
import json
import os

warnings.filterwarnings('ignore')
decord.bridge.set_bridge('torch')

"""# Defined Class"""

@dataclass
class VideoSegmentMetadata:
    """Metadata for each video segment."""
    segment_id: int
    start_time: float
    end_time: float
    num_frames: int
    fps: float


@dataclass
class CaptionResult:
    """Result container for generated captions."""
    segment_id: int
    start_time: float
    end_time: float
    caption: str

print(" Data classes defined!")

# Video Segment Extractor Class

class VideoSegmentExtractor:
    """Handles video segmentation and frame extraction."""

    def __init__(
        self,
        segment_duration: float = 30.0,
        frames_per_segment: int = 1
    ):
        self.segment_duration = segment_duration
        self.frames_per_segment = frames_per_segment

    def get_video_info(self, video_path: str) -> Dict:
        """Extract basic video information."""
        if not Path(video_path).exists():
            raise FileNotFoundError(f"Video file not found: {video_path}")

        try:
            vr = VideoReader(video_path, ctx=cpu(0))

            info = {
                'total_frames': len(vr),
                'fps': vr.get_avg_fps(),
                'duration': len(vr) / vr.get_avg_fps(),
                'width': vr[0].shape[1],
                'height': vr[0].shape[0]
            }

            print(f" Video info extracted")
            print(f"  Duration: {info['duration']:.2f} seconds")
            print(f"  FPS: {info['fps']:.2f}")
            print(f"  Resolution: {info['width']}x{info['height']}")
            print(f"  Total frames: {info['total_frames']}")

            return info

        except Exception as e:
            raise RuntimeError(f"Failed to extract video info: {str(e)}")

    def extract_video_segments(
        self,
        video_path: str
    ) -> List[Tuple[List[Image.Image], VideoSegmentMetadata]]:
        """Extract video segments with representative frames."""
        try:
            vr = VideoReader(video_path, ctx=cpu(0))
            total_frames = len(vr)
            fps = vr.get_avg_fps()
            duration = total_frames / fps

            num_segments = int(np.ceil(duration / self.segment_duration))

            segments = []

            for seg_id in range(num_segments):
                start_time = seg_id * self.segment_duration
                end_time = min((seg_id + 1) * self.segment_duration, duration)

                start_frame = int(start_time * fps)
                end_frame = int(end_time * fps)

                # Get middle frame of segment
                middle_frame = (start_frame + end_frame) // 2
                middle_frame = min(middle_frame, total_frames - 1)

                # Extract frame
                frame_tensor = vr[middle_frame]
                frame_np = frame_tensor.numpy()
                pil_image = Image.fromarray(frame_np)

                # Resize if too large
                max_size = 512
                if max(pil_image.size) > max_size:
                    ratio = max_size / max(pil_image.size)
                    new_size = tuple(int(dim * ratio) for dim in pil_image.size)
                    pil_image = pil_image.resize(new_size, Image.LANCZOS)

                frames = [pil_image]

                metadata = VideoSegmentMetadata(
                    segment_id=seg_id,
                    start_time=start_time,
                    end_time=end_time,
                    num_frames=len(frames),
                    fps=fps
                )

                segments.append((frames, metadata))

            print(f" Video segmented into {len(segments)} parts")
            print(f"  Segment duration: {self.segment_duration}s")
            print(f"  Frames per segment: {self.frames_per_segment}")

            return segments

        except Exception as e:
            print(f" Error details: {str(e)}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"Failed to extract video segments: {str(e)}")

print(" VideoSegmentExtractor class defined!")

"""# Video Captioner using BLIP-2 from Hugging Face Model"""

# BLIP-2 Video Captioner Class

class BLIP2VideoCaptioner:
    """
    Handles video captioning using BLIP-2 model.
    More stable and reliable than SmolVLM2.
    """

    def __init__(
        self,
        model_name: str = "Salesforce/blip2-opt-2.7b",
        device: Optional[str] = None
    ):
        """Initialize the BLIP-2 captioner model."""
        self.model_name = model_name

        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        self.processor = None
        self.model = None

        print(f"Loading {model_name} on {self.device}...")
        print(" This may take a few minutes on first run...")

        try:
            self.processor = Blip2Processor.from_pretrained(self.model_name)

            self.model = Blip2ForConditionalGeneration.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )

            if self.device == "cpu":
                self.model = self.model.to(self.device)

            self.model.eval()

            print(f" Model loaded successfully")
            print(f"  Model: {self.model_name}")
            print(f"  Device: {self.device}")
            print(f"  Parameters: ~{sum(p.numel() for p in self.model.parameters()) / 1e9:.2f}B")

        except Exception as e:
            print(f" Failed to load model: {str(e)}")
            raise RuntimeError(f"Failed to load model: {str(e)}")

    def generate_caption(
        self,
        frames: List[Image.Image],
        prompt: str = "a video of",
        max_new_tokens: int = 50
    ) -> str:
        """Generate caption for video segment."""
        if self.model is None or self.processor is None:
            return "[Model not initialized]"

        if not frames:
            return "[No frames provided]"

        try:
            image = frames[0]

            inputs = self.processor(
                images=image,
                text=prompt,
                return_tensors="pt"
            ).to(self.device)

            # Generate caption
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    num_beams=3,
                    do_sample=False,
                    temperature=1.0
                )

            # Decode output
            generated_text = self.processor.decode(
                generated_ids[0],
                skip_special_tokens=True
            ).strip()

            # Clean up output
            if generated_text.startswith(prompt):
                generated_text = generated_text[len(prompt):].strip()

            return generated_text if generated_text else "[No caption generated]"

        except Exception as e:
            print(f" Caption generation error: {str(e)}")
            import traceback
            traceback.print_exc()
            return f"[Error: {str(e)[:50]}]"

print(" BLIP2VideoCaptioner class defined!")

"""**Penjelasan perubahan model**: Model yang digunakan pada sistem video captioning ini mengalami perubahan dari SmolVLM2 ke BLIP-2. Pada implementasi awal, SmolVLM2 digunakan sebagai model vision-language untuk menghasilkan caption dari video, namun dalam proses pengujian ditemukan bahwa model tersebut kurang stabil saat melakukan inference, terutama ketika menangani video dengan durasi panjang atau jumlah frame yang cukup banyak. Kondisi ini menyebabkan error yang tidak konsisten serta hasil caption yang kurang reliabel.

Untuk mengatasi permasalahan tersebut, model kemudian diganti menggunakan BLIP-2 yang tersedia melalui Hugging Face Transformers. BLIP-2 memiliki integrasi yang lebih matang dengan library transformers, sehingga proses pemrosesan input visual dan generasi teks dapat berjalan dengan lebih stabil dan konsisten. Selain itu, BLIP-2 mendukung optimasi penggunaan GPU melalui pengaturan tipe data dan device mapping, yang membuat proses generate caption menjadi lebih efisien di lingkungan seperti Google Colab.
"""

# Caption Merger Class

class CaptionMerger:
    """Merges captions from video segments."""

    @staticmethod
    def merge_captions(
        caption_results: List[CaptionResult],
        add_timestamps: bool = True
    ) -> str:
        """Merge segment captions into single description."""
        if not caption_results:
            return ""

        # Sort by start time
        sorted_results = sorted(caption_results, key=lambda x: x.start_time)

        merged_sections = []

        for result in sorted_results:
            caption = result.caption.strip()

            # Skip empty or error captions
            if not caption or caption.startswith("[") or len(caption) < 3:
                continue

            if add_timestamps:
                time_marker = f"[{result.start_time:.1f}s - {result.end_time:.1f}s]"
                merged_sections.append(f"{time_marker} {caption}")
            else:
                merged_sections.append(caption)

        final_caption = "\n\n".join(merged_sections)

        print(f" Captions merged successfully")
        print(f"  Total segments: {len(sorted_results)}")
        print(f"  Valid captions: {len(merged_sections)}")
        print(f"  Final length: {len(final_caption)} characters")

        return final_caption

print(" CaptionMerger class defined!")

# Video Caption Pipeline Class

class VideoCaptionPipeline:
    """End-to-end pipeline for video captioning."""

    def __init__(
        self,
        model_name: str = "Salesforce/blip2-opt-2.7b",
        segment_duration: float = 30.0,
        device: Optional[str] = None
    ):
        """Initialize the pipeline."""
        self.segment_extractor = VideoSegmentExtractor(
            segment_duration=segment_duration,
            frames_per_segment=1
        )
        self.captioner = BLIP2VideoCaptioner(model_name, device)
        self.merger = CaptionMerger()

    def process_video(
        self,
        video_path: str,
        prompt: str = "a video of",
        add_timestamps: bool = True,
        verbose: bool = True
    ) -> Dict:
        """Process video end-to-end to generate captions."""
        if verbose:
            print("="*60)
            print("VIDEO CAPTIONING PIPELINE")
            print("Using BLIP-2 Model")
            print("="*60)
            print()

        # Step 1: Extract video info
        if verbose:
            print("Step 1: Extracting video information...")
        video_info = self.segment_extractor.get_video_info(video_path)
        print()

        # Step 2: Extract segments
        if verbose:
            print("Step 2: Extracting video segments...")
        segments = self.segment_extractor.extract_video_segments(video_path)
        print()

        # Step 3: Generate captions
        if verbose:
            print("Step 3: Generating video captions...")
        caption_results = []

        for i, (frames, metadata) in enumerate(segments):
            if verbose:
                print(f"  Processing segment {i+1}/{len(segments)} "
                      f"({metadata.start_time:.1f}s - {metadata.end_time:.1f}s)...", end=" ")

            try:
                caption = self.captioner.generate_caption(
                    frames,
                    prompt=prompt
                )

                result = CaptionResult(
                    segment_id=metadata.segment_id,
                    start_time=metadata.start_time,
                    end_time=metadata.end_time,
                    caption=caption
                )

                caption_results.append(result)

                if verbose:
                    preview = caption[:500] + "..." if len(caption) > 500 else caption
                    print(f" [{preview}]")
            except Exception as e:
                print(f" Error: {str(e)}")
                result = CaptionResult(
                    segment_id=metadata.segment_id,
                    start_time=metadata.start_time,
                    end_time=metadata.end_time,
                    caption="[Error generating caption]"
                )
                caption_results.append(result)

        print()

        # Step 4: Merge captions
        if verbose:
            print("Step 4: Merging captions...")
        final_caption = self.merger.merge_captions(
            caption_results,
            add_timestamps=add_timestamps
        )
        print()

        # Count valid captions
        valid_captions = len([r for r in caption_results
                             if r.caption and not r.caption.startswith("[")
                             and len(r.caption) > 3])

        if verbose:
            print("="*60)
            print("PROCESSING SUMMARY")
            print("="*60)
            print(f" Video duration: {video_info['duration']:.2f} seconds")
            print(f" Total segments: {len(segments)}")
            print(f" Valid captions: {valid_captions}/{len(caption_results)}")
            print(f" Final caption length: {len(final_caption)} characters")
            print()

        return {
            'final_caption': final_caption,
            'segment_results': caption_results,
            'metadata': {
                'video_info': video_info,
                'num_segments': len(segments),
                'valid_captions': valid_captions,
                'segment_duration': self.segment_extractor.segment_duration,
                'model': self.captioner.model_name
            }
        }

print(" VideoCaptionPipeline class defined!")

"""# Code Execution for Testing Video Captioning

"""

# Main Execution
from google.colab import files

def main():
    """Main function to execute the video captioning pipeline."""

    print("="*60)
    print("VIDEO CAPTIONING WITH BLIP-2")
    print("="*60)
    print()

    # Step 1: Upload video
    print(" Please upload your video file...")
    uploaded = files.upload()

    if not uploaded:
        print(" No file uploaded. Exiting.")
        return

    video_filename = list(uploaded.keys())[0]
    video_path = os.path.join(os.getcwd(), video_filename)

    print(f" File uploaded: {video_filename}")
    print(f" File size: {os.path.getsize(video_path) / (1024*1024):.2f} MB")
    print()

    # Step 2: Initialize pipeline
    print(" Initializing pipeline...")
    print(" First run may take 3-5 minutes to download model...")
    try:
        pipeline = VideoCaptionPipeline(
            model_name="Salesforce/blip2-opt-2.7b",
            segment_duration=30.0,
            device=None
        )
        print()
    except Exception as e:
        print(f" Failed to initialize pipeline: {str(e)}")
        import traceback
        traceback.print_exc()
        return

    # Step 3: Process video
    try:
        result = pipeline.process_video(
            video_path=video_path,
            prompt="a video of",
            add_timestamps=True,
            verbose=True
        )

        # Check if we got any captions
        if not result['final_caption'] or len(result['final_caption']) < 10:
            print(" WARNING: No captions generated!")
            print("This might be due to:")
            print("  1. Model initialization issue")
            print("  2. Video format incompatibility")
            print("  3. Insufficient memory")
            print("\nPlease check the error messages above.")

        # Step 4: Display results
        print("="*60)
        print("FINAL VIDEO CAPTION")
        print("="*60)
        print(result['final_caption'] if result['final_caption'] else "(No captions generated)")
        print()

        # Step 5: Create JSON output
        json_output = {
            "nama_file": video_filename,
            "caption": result['final_caption']
        }

        # Step 6: Save to JSON file
        output_json_filename = f"{os.path.splitext(video_filename)[0]}_caption.json"
        with open(output_json_filename, 'w', encoding='utf-8') as f:
            json.dump(json_output, f, ensure_ascii=False, indent=2)

        # Display JSON output
        print("="*60)
        print("JSON OUTPUT")
        print("="*60)
        print(json.dumps(json_output, ensure_ascii=False, indent=2))
        print()

        print("="*60)
        print(f" JSON saved to: {output_json_filename}")
        print("="*60)

        # Step 7: Download JSON file
        print("\n Downloading JSON file...")
        files.download(output_json_filename)

        if result['final_caption'] and len(result['final_caption']) > 10:
            print("\n Process completed successfully!")
        else:
            print("\n Process completed but captions are empty. See warnings above.")

    except Exception as e:
        print(f" Pipeline failed: {str(e)}")
        import traceback
        traceback.print_exc()

    finally:
        # Cleanup uploaded file
        if os.path.exists(video_path):
            print(f"\n Cleaning up: {video_filename}")
            os.remove(video_path)

if __name__ == "__main__":
    main()

"""**Penjelasan insight:** Video yang diupload dari device user bisa digenerate menjadi video captioning yang disimpan dalam bentuk .json
dengan menggunakan BLIP-2 dari Hugging Face model, sistem video captioning menunjukkan peningkatan dari sisi stabilitas, kualitas caption, dan keandalan pipeline secara keseluruhan. Caption yang dihasilkan lebih deskriptif dan koheren terhadap konten visual video, sehingga model ini dinilai lebih sesuai untuk digunakan dalam pengembangan dan evaluasi sistem captioning video.
"""