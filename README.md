# ğŸ¥ Video Captioning using BLIP-2 (Hugging Face)

This repository implements an **end-to-end video captioning pipeline** using the **BLIP-2 multimodal model from Hugging Face**.  
The system converts videos into descriptive text by **extracting representative frames** and generating captions using a powerful vision-language model.

---

## âœ¨ Features

- ğŸ”¹ Video segmentation and frame sampling
- ğŸ”¹ Image-to-text captioning using **BLIP-2 (Salesforce)**
- ğŸ”¹ Support for long videos via segment-based processing
- ğŸ”¹ Modular and easy-to-extend pipeline
- ğŸ”¹ Works on Google Colab and local environments
- ğŸ”¹ Hugging Face Transformers integration

---

## ğŸ§  Model Used

- **BLIP-2 (Bootstrapping Language-Image Pre-training)**
- Provider: **Salesforce Research**
- Hosted on: **Hugging Face**

Supported models:
- `Salesforce/blip2-flan-t5-xl`
- `Salesforce/blip2-opt-2.7b`

> âš ï¸ BLIP-2 is an image-captioning model.  
> For video captioning, videos are decomposed into frames which are captioned and then aggregated.

---

## ğŸ—ï¸ Pipeline Overview

```text
Video
 â””â”€â”€ Segment video (time-based)
      â””â”€â”€ Sample frames per segment
           â””â”€â”€ BLIP-2 caption per frame / segment
                â””â”€â”€ Merge captions into video-level description
